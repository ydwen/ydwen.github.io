
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Yandong Wen</title>
  <link rel="shortcut icon" href="src/logo.jpg ">
  <meta content="Yandong Wen" name="keywords">
  <style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #01579B;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 830px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  background: #F4F6F6;
}

h2 {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  /* font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 13px;*/
  font-weight: bold;
}

ul { 
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}


alert {
  font-family: Arial, Helvetica, sans-serif;
  /*font-size: 13px;*/
  color: #FF0000;
}

em, i {
  font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.2em;
  background: #F4F6F6;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.8em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
  /* box-shadow: 0px 0px 3px #DFDFDF; */
}
div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.paper:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}

div.paper2:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}
div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
  /* box-shadow: 0px 0px 3px #DFDFDF; */
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 13px;
  margin: 1em 0;
  padding: 0;
}

    .bot {
  font-size: 14%;
}

   .ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #428bca;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
/* navigation */
#nav {
  /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
       /* Corbel, Arial, Helvetica, sans-serif; */
  font-family: Georgia, Helvetica, sans-serif;
  position: fixed;
  top: 50px;
  /* left: 860px; */
  margin-left: 830px;     /*1060*/
  width: 93px;
  font-size: 14px;
}

#nav li2 {
    margin-bottom: 1px;
}
ol {
  list-style: none;
}
#nav a {
    display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
    /* background-color: #242d36 ; */
}

</style>

<script type="text/javascript" async="" src="./src/ga.js"></script><script type="text/javascript" async="" src="./src/ga(1).js"></script><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<script type="text/javascript" src="./src/hidebib.js"></script>
<script src="./src/main.js"></script>
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>


<body>
<ol id="nav">
    <li><a href="https://ydwen.github.io/" title="Home">Home</a></li>
    <!-- <li><a href="https://ydwen.github.io/#pub" title="Papers">Papers</a></li> -->
    <!-- // <li><a href="index.html"  title="Selected" id="select1" onclick="showPubs(1); return false;">Selected</a></li> -->
    <!--<li><a href="#award" title="Awards">Awards</a></li>-->
</ol>

<a name="home"></a>
<div style="margin-bottom: 1em; border-top-right-radius:10px; border-top-left-radius:10px; border-bottom-left-radius:10px; border-bottom-right-radius:10px; border: 0px solid #ddd; background-color: #fff; padding: 1em; height: 140px; /* box-shadow: 0px 0px 3px #DFDFDF; */">
<div style="margin: 0px auto; width: 100%;">
  <img title="" style="float: left; padding-left: .3em; padding-top: 0em; height: 140px;" src="./src/wen.jpg">
  <div style="padding-left: 16em; padding-top: 0em; vertical-align: top; height: 0px; width: 100%;">
    <p>&nbsp;</p>
    <p><span style="line-height: 80%; font-size: 16pt;">Yandong Wen</span></p>
    <p>&nbsp;</p>
    <p><a href="https://scholar.google.com/citations?user=HmylMfcAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;&nbsp;<a href="https://github.com/ydwen">Github</a>&nbsp;&nbsp;&nbsp;<a href="YandongWen_CV.pdf">CV</a></p>
    <p>&nbsp;</p>
    <p>Perceiving Systems<br>Max Planck Institute for Intelligent Systems</p>
  </div>
</div>
</div>

<div style="clear: both;">

<div class="section">
<h2>About Me</h2>
<div class="bio" id="biosection">
  <p style="margin-bottom:9px">I am currently a post-doctoral researcher at <a href="https://is.mpg.de/">MPI TÃ¼bingen</a>, working with <a href="https://ps.is.mpg.de/person/black">Michael J. Black</a>. My research focuses on understanding the human face and voice, including aspects such as recognition, reconstruction, and generation. Specifically, I have worked on face and speaker recognition, cross-modal recognition, as well as face and cross-modal reconstruction and generation.</p>

  <p style="margin-bottom:9px">Before joining MPI-IS, I completed my PhD at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, where I was advised by Prof. <a href="http://mlsp.cs.cmu.edu/people/rsingh/">Rita Singh</a> and <a href="http://mlsp.cs.cmu.edu/people/bhiksha/">Bhiksha Raj</a>. I was a member of <a href="http://mlsp.cs.cmu.edu/">MLSP Group</a>. Prior to that, I received my master's and bachelor's degrees from <a href="https://www.scut.edu.cn/en/">South China University of Technology</a>, advised by <a href="https://www.researchgate.net/profile/Yuli-Fu/">Yuli Fu</a>. I have also spent time at <a href="https://tech.facebook.com/reality-labs/2020/2/facebook-reality-labs-inside-our-new-pittsburgh-digs/">Facebook Reality Lab</a> and <a href="http://english.siat.cas.cn/">Shenzhen Institute of Advanced Technology</a>.</p>
</div>
</div>
</div>


<a name="pub"></a>
<div style="clear: both;">
<div class="section">
<div style="clear: both;">
  <div class="section">
    <h2 id="confpapers2"> Publications</h2><!--<a href="index.html" id="select1" onclick="showPubs(1); return false;" title="Papers"></a><a href="index.html" id="select2" onclick="showPubs(2); return false;" ></a>-->

    <div class="paper" id="liu2023boft">
      <p><a href="https://arxiv.org/abs/2311.06243"><b>Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization</b></a><br>
        Weiyang Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution &amp; Project Lead">*</a>, Zeju Qiu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Yao Feng<a href="https://ydwen.github.io/#star" title="Equal Contribution">**</a>, Yuliang Xiu<a href="https://ydwen.github.io/#star" title="Equal Contribution">**</a>, Yuxuan Xue<a href="https://ydwen.github.io/#star" title="Equal Contribution">**</a>, Longhui Yu<a href="https://ydwen.github.io/#star" title="Equal Contribution">**</a>, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, <u>Yandong Wen</u>, Michael J. Black, Adrian Weller, Bernhard SchÃ¶lkopf</p>
      <p><em><font color="#B71C1C">Preprint 2023</font></em></p>
      <p><a href="papers/LiuPreprint23.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/wy1iu/butterfly-oft">code</a>&nbsp;|&nbsp;<a href="https://boft.wyliu.com/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;liu2023boft&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @article{liu2023boft,
      author = {Liu, Weiyang and Qiu, Zeju and Feng, Yao and Xiu, Yuliang and Xue, Yuxuan and Yu, Longhui and Feng, Haiwen and Liu, Zhen 
        and Heo, Juyeon and Peng, Songyou and Wen, Yandong and Black, Michael J. and Weller, Adrian and Sch{\"o}lkopf, Bernhard},
      title = {Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization},
      journal = {arXiv},
      year = {2023}}</pre>
    </div>


    <div class="paper" id="zhang2023text">
      <p><a href="https://arxiv.org/abs/2309.07125"><b>Text-Guided Generation and Editing of Compositional 3D Avatars</b></a><br>
        Hao Zhang, Yao Feng, Peter Kulits, <u>Yandong Wen</u>, Justus Thies, Michael J. Black</p>
      <p><em><font color="#B71C1C">3DV 2023</font></em></p>
      <p><a href="papers/Zhang3DV23.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/HaoZhang990127/TECA">code</a>&nbsp;|&nbsp;<a href="https://yfeng95.github.io/teca/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;zhang2023text&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{zhang2023text,
      title={Text-Guided Generation and Editing of Compositional},
      author={Zhang, Hao and Feng, Yao and Kulits, Peter and Wen, Yandong and Thies, Justus and Black, Michael J.},
      booktitle={3DV},
      year={2023}}</pre>
    </div>


    <div class="paper" id="li2023rethinking">
      <p><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611779"><b>Rethinking Voice-Face Correlation: A Geometry View</b></a><br>
        Xiang Li, <u>Yandong Wen</u>, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj</p>
      <p><em><font color="#B71C1C">ACM MM 2023</font></em></p>
      <p><a href="papers/LiACMMM23.pdf">paper</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;li2023rethinking&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{li2023rethinking,
      title={Rethinking Voice-Face Correlation: A Geometry View},
      author={Li, Xiang and Wen, Yandong and Yang, Muqiao and Wang, Jinglu and Singh, Rita and Raj, Bhiksha},
      booktitle={ACMMM},
      year={2023}}</pre>
    </div>


    <div class="paper" id="qu2023hidden">
      <p><a href="https://www.isca-speech.org/archive/interspeech_2023/qu23_interspeech.html"><b>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features</b></a><br>
        Liao Qu, Xianwei Zou, Xiang Li, <u>Yandong Wen</u>, Rita Singh, Bhiksha Raj</p>
      <p><em><font color="#B71C1C">InterSpeech 2023</font></em></p>
      <p><a href="papers/QuInterSpeech23.pdf">paper</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;qu2023hidden&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{qu2023hidden,
      title={The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features},
      author={Qu, Liao and Zou, Xianwei and Li, Xiang and Wen, Yandong and Singh, Rita and Raj, Bhiksha},
      booktitle={InterSpeech},
      year={2023}}</pre>
    </div>


    <div class="paper" id="danvevcek2023emotional">
      <p><a href="https://arxiv.org/abs/2306.08990"><b>Emotional Speech-Driven Animation with Content-Emotion Disentanglement</b></a><br>
        Radek DanÄÄek, Kiran Chhatre, Shashank Tripathi, <u>Yandong Wen</u>, Michael J. Black, Timo Bolkart</p>
      <p><em><font color="#B71C1C">SIGGRAPH Asia 2023</font></em></p>
      <p><a href="papers/DanvevcekSigAsia23.pdf">paper</a>&nbsp;|&nbsp;<a href="https://emote.is.tue.mpg.de/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;danvevcek2023emotional&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{danvevcek2023emotional,
      title={Emotional Speech-Driven Animation with Content-Emotion Disentanglement},
      author={Dan{\v{e}}{\v{c}}ek, Radek and Chhatre, Kiran and Tripathi, Shashank
       and Wen, Yandong and Black, Michael J. and Bolkart, Timo},
      booktitle={Siggraph Asia},
      year={2023}}</pre>
    </div>


    <div class="paper" id="wen2023simple">
      <p><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.html"><b>Pairwise Similarity Learning is SimPLE</b></a><br>
        <u>Yandong Wen</u><a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Weiyang Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard SchÃ¶lkopf</p>
      <p><em><font color="#B71C1C">ICCV 2023</font></em></p>
      <p><a href="papers/WenICCV23.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/ydwen/opensphere">code</a>&nbsp;|&nbsp;<a href="https://simple.is.tue.mpg.de/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2023simple&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2023simple,
      title={Pairwise Similarity Learning is SimPLE},
      author={Wen, Yandong and Liu, Weiyang and Feng, Yao and Raj, Bhiksha and Singh, Rita
        and Weller, Adrian and Black, Michael and Sch{\"o}lkopf, Bernhard},
      booktitle={ICCV},
      year={2023}}</pre>
    </div>

    <div class="paper" id="yi2023generating">
      <p><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.html"><b>Generating Holistic 3D Human Motion from Speech</b></a><br>
        Hongwei Yi<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Hualin Liang<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Yifei Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Qiong Cao, <u>Yandong Wen</u>, Timo Bolkart, Dacheng Tao, Michael J. Black</p>
      <p><em><font color="#B71C1C">CVPR 2023</font></em></p>
      <p><a href="papers/YiCVPR23.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/yhw-yhw/TalkSHOW">code</a>&nbsp;|&nbsp;<a href="https://talkshow.is.tue.mpg.de/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;yi2023generating&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{yi2023generating,
      title={Generating Holistic 3D Human Motion from Speech},
      author={Yi, Hongwei and Liang, Hualin and Liu, Yifei and Cao, Qiong and Wen, Yandong 
        and Bolkart, Timo and Tao, Dacheng and Black, Michael J.},
      booktitle={CVPR},
      year={2023}}</pre>
    </div>


    <div class="paper" id="liu2023spherefacer">
      <p><a href="https://ieeexplore.ieee.org/document/9736575"><b>SphereFace Revived: Unifying Hyperspherical Face Recognition</b></a><br>
        Weiyang Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, <u>Yandong Wen</u><a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Bhiksha Raj, Rita Singh, Adrian Weller</p>
      <p><em><font color="#B71C1C">TPAMI 2022</font></em></p>
      <p><a href="papers/LiuPAMI22.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/ydwen/opensphere">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;liu2023spherefacer&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @article{liu2023spherefacer,
      title={SphereFace Revived: Unifying Hyperspherical Face Recognition},
      author={Liu, Weiyang and Wen, Yandong and Raj, Bhiksha and Singh, Rita and Weller, Adrian},
      journal={IEEE Transactions on Pattern Analysis \&amp; Machine Intelligence},
      volume={45},
      number={02},
      pages={2458--2474},
      year={2023}}</pre>
    </div>


    <div class="paper" id="wen2021sphereface2">
      <p><a href="https://openreview.net/forum?id=l3SDgUh7qZO"><b>SphereFace2: Binary Classification is All You Need for Deep Face Recognition</b></a><br>
        <u>Yandong Wen</u><a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Weiyang Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Adrian Weller, Bhiksha Raj, Rita Singh</p>
      <p><em><font color="#B71C1C">ICLR 2022</font>&nbsp;&nbsp;&nbsp;<font color="#239B56">Spotlight</font></em></p>
      <p><a href="papers/WenICLR22.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/ydwen/opensphere">code</a>&nbsp;|&nbsp;<a href="https://opensphere.world/">project</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2021sphereface2&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2021sphereface2,
      title={SphereFace2: Binary Classification is All You Need for Deep Face Recognition},
      author={Wen, Yandong and Liu, Weiyang and Weller, Adrian and Raj, Bhiksha and Singh, Rita},
      booktitle = {ICLR},
      year={2021}}</pre>
    </div>


    <div class="paper" id="richard2021meshtalk">
      <p><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.html"><b>MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement</b></a><br>
        Alexander Richard, Michael Zollhoefer, <u>Yandong Wen</u>, Fernando de la Torre, Yaser Sheikh</p>
      <p><em><font color="#B71C1C">ICCV 2021</font></em></p>
      <p><a href="papers/RichardICCV21.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/facebookresearch/meshtalk">code</a>&nbsp;|&nbsp;<a href="https://www.facebook.com/MetaResearch/videos/251508987094387/">talk</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;richard2021meshtalk&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{richard2021meshtalk,
      title={MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement},
      author={Richard, Alexander and Zollh{\"o}fer, Michael and Wen, Yandong and De la Torre, Fernando and Sheikh, Yaser},
      booktitle={ICCV},
      year={2021}}</pre>
    </div>


    <div class="paper" id="wen2021self">
      <p><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wen_Self-Supervised_3D_Face_Reconstruction_via_Conditional_Estimation_ICCV_2021_paper.html"><b>Self-Supervised 3D Face Reconstruction via Conditional Estimation</b></a><br>
        <u>Yandong Wen</u>, Weiyang Liu, Bhiksha Raj, Rita Singh</p>
      <p><em><font color="#B71C1C">ICCV 2021</font></em></p>
      <p><a href="papers/WenICCV21.pdf">paper</a>&nbsp;|&nbsp;<a href="https://youtu.be/Rw0wzEdu3Yc">talk</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2021self&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2021self,
      title={Self-Supervised 3D Face Reconstruction via Conditional Estimation},
      author={Wen, Yandong and Liu, Weiyang and Raj, Bhiksha and Singh, Rita},
      booktitle={ICCV},
      year={2021}}</pre>
    </div>


    <div class="paper" id="wen2019comprehensive">
      <p><a href="https://link.springer.com/article/10.1007/s11263-018-01142-4"><b>A Comprehensive Study on Center Loss for Deep Face Recognition</b></a><br>
        <u>Yandong Wen</u><a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Kaipeng Zhang<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Zhifeng Li, Yu Qiao</p>
      <p><em><font color="#B71C1C">IJCV 2019</font></em></p>
      <p><a href="papers/WenIJCV2019.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/ydwen/centerloss">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2019comprehensive&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @article{wen2019comprehensive,
      title={A Comprehensive Study on Center Loss for Deep Face Recognition},
      author={Wen, Yandong and Zhang, Kaipeng and Li, Zhifeng and Qiao, Yu},
      journal={International Journal of Computer Vision},
      volume={127},
      pages={668--683},
      year={2019}}</pre>
    </div>


    <div class="paper" id="wen2019face">
      <p><a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/eb9fc349601c69352c859c1faa287874-Abstract.html"><b>Face Reconstruction from Voice using Generative Adversarial Networks</b></a><br>
        <u>Yandong Wen</u>, Rita Singh, and Bhiksha Raj</p>
      <p><em><font color="#B71C1C">NeurIPS 2019</font></em></p>
      <p><a href="papers/WenNeurIPS19.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/cmu-mlsp/reconstructing_faces_from_voices">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2019face&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2019face,
      title={Face Reconstruction from Voice using Generative Adversarial Networks},
      author={Wen, Yandong and Raj, Bhiksha and Singh, Rita},
      booktitle={NeurIPS},
      year={2019}}</pre>
    </div>


    <div class="paper" id="wen2018disjoint">
      <p><a href="https://openreview.net/forum?id=B1exrnCcF7"><b>Disjoint Mapping Network for Cross-modal Matching of Voices and Faces</b></a><br>
        <u>Yandong Wen</u>, Mahmoud Alismail, Weiyang Liu, Bhiksha Raj, Rita Singh</p>
      <p><em><font color="#B71C1C">ICLR 2019</font></em></p>
      <p><a href="papers/WenICLR19.pdf">paper</a>&nbsp;|&nbsp;<a href="https://drive.google.com/file/d/1PRBevtrWWLqKb4kd3Bpv2rTaudkBFnSO/view?usp=sharing">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2018disjoint&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2018disjoint,
      title={Disjoint Mapping Network for Cross-modal Matching of Voices and Faces},
      author={Wen, Yandong and Alismail, Mahmoud and Liu, Weiyang and Raj, Bhiksha and Singh, Rita},
      booktitle={ICLR},
      year={2019}}</pre>
    </div>


    <div class="paper" id="wen2018corrective">
      <p><a href="https://ieeexplore.ieee.org/document/8461340"><b>A Corrective Learning Approach for Text-Independent Speaker Verification</b></a><br>
        <u>Yandong Wen</u>, Tianyan Zhou; Rita Singh; Bhiksha Raj</p>
      <p><em><font color="#B71C1C">ICASSP 2019</font></em></p>
      <p><a href="papers/WenICASSP18.pdf">paper</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2018corrective&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{wen2018corrective,
      title={A Corrective Learning Approach for Text-Independent Speaker Verification},
      author={Wen, Yandong and Zhou, Tianyan and Singh, Rita and Raj, Bhiksha},
      booktitle={ICASSP},
      year={2018}}</pre>
    </div>


    <div class="paper2" id="zhang2017range">
      <p><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Range_Loss_for_ICCV_2017_paper.html"><b>Range Loss for Deep Face Recognition with Long-tailed Training Data</b></a><br>
        Xiao Zhang, Zhiyuan Fang, <u>Yandong Wen</u>, Zhifeng Li, Yu Qiao<br>
      <em><font color="#B71C1C">ICCV 2017</font>&nbsp;</em></p>
      <p><a href="papers/ZhangICCV17.pdf">paper</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;zhang2017range&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{zhang2017range,
      title = {Range loss for deep face recognition with long-tailed training data},
      author = {Zhang, Xiao and Fang, Zhiyuan and Wen, Yandong and Li, Zhifeng and Qiao, Yu},
      booktitle = {ICCV},
      year = {2017}}</pre>
    </div>


    <div class="paper2" id="liu2017sphereface">
      <p><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.html"><b>SphereFace: Deep Hypersphere Embedding for Face Recognition</b></a><br>
        Weiyang Liu, <u>Yandong Wen</u>, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song<br>
      <em><font color="#B71C1C">CVPR 2017</font>&nbsp;&nbsp;&nbsp;<a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/" style="color:#239B56;">Paper Digest Most Influential Paper</a></em></p>
      <p><a href="papers/LiuCVPR17.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/wy1iu/sphereface">code</a>&nbsp;|&nbsp;<a href="https://youtu.be/P6jEzzwoYWs">demo</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;liu2017sphereface&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @InProceedings{liu2017sphereface,
      title = {SphereFace: Deep Hypersphere Embedding for Face Recognition},
      author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
      booktitle = {CVPR},
      year = {2017}}</pre>
    </div>


    <div class="paper2" id="wen2016discriminative">
      <p><a href="https://link.springer.com/chapter/10.1007/978-3-319-46478-7_31"><b>A Discriminative Deep Feature Learning Approach for Face Recognition</b></a><br>
        <u>Yandong Wen</u>, Kaipeng Zhang, Zhifeng Li and Yu Qiao<br>
      <em><font color="#B71C1C">ECCV 2016</font></em></p>
      <p><a href="papers/WenECCV16.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/ydwen/caffe-face">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2016discriminative&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @inproceedings{wen2016discriminative,
      title={A Discriminative Deep Feature Learning Approach for Face Recognition},
      author={Wen, Yandong and Zhang, Kaipeng and Li, Zhifeng and Qiao, Yu},
      booktitle={ECCV},
      year={2016}}</pre>
    </div>


    <div class="paper2" id="liu2016large">
      <p><a href="https://proceedings.mlr.press/v48/liud16.html"><b>Large-Margin Softmax Loss for Convolutional Neural Networks</b></a><br>
        Weiyang Liu<a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, <u>Yandong Wen</u><a href="https://ydwen.github.io/#star" title="Equal Contribution">*</a>, Zhiding Yu, Meng Yang<br>
      <em><font color="#B71C1C">ICML 2016</font></em></p>
      <p><a href="papers/LiuICML16.pdf">paper</a>&nbsp;|&nbsp;<a href="https://github.com/wy1iu/LargeMargin_Softmax_Loss">code</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;liu2016large&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @inproceedings{liu2016large,
      title={Large-Margin Softmax Loss for Convolutional Neural Networks},
      author={Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
      booktitle={ICML},
      year={2016}}</pre>
    </div>


    <div class="paper2" id="wen2016latent">
      <p><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Wen_Latent_Factor_Guided_CVPR_2016_paper.html"><b>Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition</b></a><br>
        <u>Yandong Wen</u>, Zhifeng Li and Yu Qiao<br>
      <em><font color="#B71C1C">CVPR 2016</font></em></p>
      <p><a href="papers/WenCVPR16.pdf">paper</a>&nbsp;|&nbsp;<a shape="rect" href="javascript:togglebib(&#39;wen2016latent&#39;)" class="togglebib">bib</a></p>
      <pre xml:space="preserve" style="display: none;">  @inproceedings{wen2016latent,
      title={Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition},
      author={Wen, Yandong and Li, Zhifeng and Qiao, Yu},
      booktitle={CVPR},
      year={2016}}</pre>
    </div>


  </div>

</div>
</div>
</div>
<script>showPubs(0);</script>

Last updated on 23th November 2023.
<!-- <div align="center">This site has been visisted <a href="https://www.easycounter.com/"><img src="./src/counter.php" width="56" height="10" border="0" alt="Free Web Counter"></a> times in total.</div> -->
<div align="center">This site has been visisted<a href="http://www.amazingcounters.com"> <img src="http://cc.amazingcounters.com/counter.php?i=3196355&c=9589378" alt="AmazingCounters.com" width="64" height="13" border="0"></a> times in total.</div>
<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=UBBK&d=QYI6-YIhAQ_rq3wdlC6X_L_DZEwv1gip6P_62-gTJUs"></script>-->



</div></body></html>